{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f24585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Jupyter Notebook Template for Take-home Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258f36b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Classification (Tabular)\n",
    "\n",
    "| Challenge type | How to recognize | Recommended split | Baseline model | Primary metrics | Must-do checks | Common pitfalls |\n",
    "|---|---|---|---|---|---|---|\n",
    "| **Binary Classification** | target has 2 classes (0/1, yes/no) | Stratified random; Time/Group if needed | Logistic Regression | ROC-AUC + LogLoss; PR-AUC if rare | prevalence, leakage scan, threshold plan | using accuracy only; ignoring threshold |\n",
    "| **Multiclass Classification** | target has >2 classes | Stratified random; Time/Group if needed | Multinomial Logistic Regression | Macro F1 + LogLoss | class balance, confusion matrix | micro-only metrics hide minorities |\n",
    "| **Highly Imbalanced Classification** | positive rate very low / “rare event” | Stratified + Time/Group if needed | Logistic (class_weight) | PR-AUC; Recall@Precision | thresholding + calibration | reporting ROC-AUC only |\n",
    "| **Event Prediction (future event)** | label defined over future window (churn/fraud) | Time split preferred | Logistic / GBM | PR-AUC or Recall@Precision | window definition, censoring | leakage from post-event features |\n",
    "| **Cost-Sensitive Classification** | prompt gives FP/FN costs | Same as above + align to decision | Logistic | expected cost / utility | cost matrix, threshold by cost | optimizing generic metric |\n",
    "| **Group Leakage / Repeated Measures** | many rows per user/account/session | Group split (GroupKFold) | Logistic | same as above | confirm group key | splitting rows not groups |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a7c11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Regression (Tabular)\n",
    "\n",
    "| Challenge type | How to recognize | Recommended split | Baseline model | Primary metrics | Must-do checks | Common pitfalls |\n",
    "|---|---|---|---|---|---|---|\n",
    "| **Regression (continuous)** | numeric target with many unique values | Random; Time/Group if needed | Ridge / ElasticNet | MAE or RMSE; R² secondary | outliers, target skew, residuals | optimizing R² only |\n",
    "| **Count Regression** | target non-negative integers (skewed) | Random/Time/Group as needed | PoissonRegressor / Ridge on log1p(y) | MAE or deviance | zeros %, transform choice | negative predictions |\n",
    "| **Time-to-Value / Horizon Regression** | target is “next week/month value” | Time split | Ridge / GBM | MAE/RMSE by horizon | leakage via future aggregates | random split inflates |\n",
    "| **Group Leakage Regression** | multiple rows per entity | Group split | Ridge | MAE/RMSE | group consistency | memorizing IDs |\n",
    "| **Missing-Heavy Regression** | lots of NaNs | Same as task | Ridge + imputer | MAE/RMSE | missingness patterns | imputing using whole dataset |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468364f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Time Series\n",
    "\n",
    "| Challenge type | How to recognize | Recommended split | Baseline model | Primary metrics | Must-do checks | Common pitfalls |\n",
    "|---|---|---|---|---|---|---|\n",
    "| **Forecasting (univariate)** | single series + dates | Rolling/expanding time split | Naive / seasonal naive | MAE/RMSE/SMAPE | seasonality, gaps | random split |\n",
    "| **Forecasting (panel)** | many entities each has series | Time split + group by entity | Per-entity baseline / Ridge with lags | MAE/RMSE/SMAPE | lag construction from past only | leakage via future lags |\n",
    "| **Time Drift / Non-stationary** | performance changes over time | Time split + time-sliced eval | Regularized linear | time-bucket metrics | drift checks | random CV inflates |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b1851",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Ranking / Recommender\n",
    "\n",
    "| Challenge type | How to recognize | Recommended split | Baseline model | Primary metrics | Must-do checks | Common pitfalls |\n",
    "|---|---|---|---|---|---|---|\n",
    "| **Ranking / Top-K Retrieval** | “rank”, “top-k”, “search” | Time split; group by user/query | LightGBM ranker / Logistic | NDCG@K, MAP@K, Recall@K | define K + protocol | mixing users across folds |\n",
    "| **Recommendation (implicit)** | user–item interactions, clicks | Time split; group by user | Popularity → MF/logistic | Recall@K / NDCG@K | negative sampling | evaluating on seen items |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa0f85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### NLP / Text\n",
    "\n",
    "| Challenge type | How to recognize | Recommended split | Baseline model | Primary metrics | Must-do checks | Common pitfalls |\n",
    "|---|---|---|---|---|---|---|\n",
    "| **Text Classification** | text column + class label | Stratified; Group by author/user if needed | TF-IDF + Logistic | F1 / ROC-AUC; LogLoss | dedup/near-dup, leakage | leakage via duplicates |\n",
    "| **Text Regression** | text → numeric | Random/Group/Time as needed | TF-IDF + Ridge | MAE/RMSE | outliers, leakage words | overfitting, heavy models |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489f67b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Unsupervised / Weakly Supervised\n",
    "\n",
    "| Challenge type | How to recognize | Recommended split | Baseline model | Primary metrics | Must-do checks | Common pitfalls |\n",
    "|---|---|---|---|---|---|---|\n",
    "| **Anomaly Detection (no labels)** | “detect anomalies”, no target | Time holdout if temporal | IsolationForest / robust z-score | Precision@K (if labels) / manual review | score + threshold strategy | no eval protocol |\n",
    "| **Survival / Time-to-Event** | time-to-event + censoring | Time split | CoxPH (if available) | C-index | censoring handling | treating as standard classification |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc56f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Cross-cutting Constraints (add-on labels)\n",
    "\n",
    "| Constraint | When it appears | What to change in workflow |\n",
    "|---|---|---|\n",
    "| **Explainability required** | “interpret”, “justify”, “stakeholders” | favor linear/GAM; coefficient stability; simple FE |\n",
    "| **Production/deployment** | “deploy”, “latency”, “monitoring” | pipeline reproducibility, simple model, logging/monitoring notes |\n",
    "| **High-cardinality categoricals** | IDs or huge unique counts | cap rare categories; hashing if needed; avoid leakage via ID memorization |\n",
    "| **Missing-heavy** | many NaNs / structured missingness | imputation inside pipeline; consider missing indicators |\n",
    "| **Cost-sensitive** | explicit FP/FN costs | optimize threshold by expected cost, not generic metric |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, KFold,\n",
    "    GroupKFold, GroupShuffleSplit, TimeSeriesSplit, cross_validate\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, log_loss, f1_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    confusion_matrix, precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f58ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## End-to-End Workflow (General Template)\n",
    "\n",
    "**Plan (time-boxed)**\n",
    "1) `Quick Overview`: define target; check data leakage; check dtypes/ missingness/ dupes; decide split (random/time/group)\n",
    "2) `Lightweight EDA` (2–3 checks only): target sanity + missingness + 1 relationship\n",
    "3) `Feature Engineering` (minimal, interpretable)\n",
    "4) `Data Split`\n",
    "5) `Baseline pipeline`: ColumnTransformer + (LogisticRegression or Ridge)\n",
    "6) `Cross-Validation` with appropriate split + metric\n",
    "7) Holdout Evaluation + quick error analysis\n",
    "8) (Optional) Calibration/thresholding (if probabilistic classification)\n",
    "9) Sanity-check with a simple non-linear model\n",
    "10) Explainability (coefficients / importances) + wrap-up summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"YOUR_FILE.csv\"\n",
    "TARGET = \"target\"\n",
    "TIME_COL = None     # e.g. \"timestamp\" (set if time leakage matters)\n",
    "GROUP_COL = None    # e.g. \"user_id\" (set if multiple rows per entity)\n",
    "DROP_COLS = []      # e.g. IDs, post-event fields, text blobs, etc.\n",
    "TASK = \"auto\"       # \"auto\" | \"classification\" | \"regression\"\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82cb78c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Quick Overview\n",
    "- define target\n",
    "- check data leakage\n",
    "- check data dtypes, missingness, dupes\n",
    "- decide data split (stratified/ time/ group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1749b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d88e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206318a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dtypes\n",
    "df.dtypes\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4109a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data missing %\n",
    "df.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5909782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target distribution\n",
    "y = df[TARGET]\n",
    "sns.histplot(data=df, x=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target value counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e61ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be3915f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Lightweight EDA\n",
    "- target distribution/ prevalence\n",
    "- data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d7ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c01e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "501148ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adf845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfd2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ddce3c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Built Pipelines - Baseline Models\n",
    "- data split (random vs. time vs. group -> avoid leakage)\n",
    "- data preprocessing (ColumnTransformor)\n",
    "- ML model (e.g. Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e564773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "114f5865",
   "metadata": {},
   "source": [
    "#### 1. Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40fd0b",
   "metadata": {},
   "source": [
    "- Ensure split first, then fit preprocessors on train: Scaling/ Encoding/ Imputation should be learned from X_train only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd676186",
   "metadata": {},
   "source": [
    "- `random split`\n",
    "- Don't use random split for time series/ grouped data: \n",
    "1. If rows are time-ordered or user level, random split can leak info.\n",
    "2. For time-ordered case => use time-based split.\n",
    "3. For user level case => use GroupShuffleSplit/ GroupKFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb49eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced target classification problem\n",
    "# stratify = y: preserve the class distribution (very important when y is rare!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c7e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other problems\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f37424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "712ac1da",
   "metadata": {},
   "source": [
    "- `time-series data split`\n",
    "- should preserve time order and ensure no leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc049750",
   "metadata": {},
   "outputs": [],
   "source": [
    "### version 1: split by row % \n",
    "df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
    "df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "train_size = 0.8\n",
    "split_idx = int(train_size * len(df))\n",
    "\n",
    "train = df.iloc[:split_idx].copy()\n",
    "test  = df.iloc[split_idx:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### version 2: split by a time cutoff\n",
    "\n",
    "df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
    "df = df.sort_values(TIME_COL)\n",
    "\n",
    "cutoff = df[TIME_COL].quantile(0.8)   # or pick an explicit date like pd.Timestamp(\"2024-10-01\")\n",
    "\n",
    "train = df[df[TIME_COL] < cutoff].copy()\n",
    "test  = df[df[TIME_COL] >= cutoff].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### version 3: per-entity cutoff (common in finance)\n",
    "\n",
    "df[TIME_COL] = pd.to_datetime(df[TIME_COL])\n",
    "df = df.sort_values([ID_COL, TIME_COL])\n",
    "\n",
    "def split_group(g, train_frac=0.8):\n",
    "    n = len(g)\n",
    "    k = int(train_frac * n)\n",
    "    return g.iloc[:k], g.iloc[k:]\n",
    "\n",
    "parts = [split_group(g) for _, g in df.groupby(ID_COL, sort=False)]\n",
    "train = pd.concat([p[0] for p in parts]).copy()\n",
    "test  = pd.concat([p[1] for p in parts]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=[TARGET])\n",
    "y_train = train[TARGET]\n",
    "X_test  = test.drop(columns=[TARGET])\n",
    "y_test  = test[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8568e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4f3dc54",
   "metadata": {},
   "source": [
    "-  `group split`\n",
    "- split by ID so that all rows from the same group stay entirely in either train or test, never both\n",
    "- the prevents leakage when rows within a group are correlated\n",
    "- if random-split rows, the model can \"cheat\" by seeing very similar samples from the same group in both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b95320",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train/ test group split (single holdout)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "groups = df[\"UserID\"]  # or SessionID / PatientID / Ticker, etc.\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=RANDOM_STATE)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### group cross-validation\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "groups = df[\"UserID\"]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "scores = cross_val_score(pipe, X, y, cv=gkf, groups=groups, scoring=\"roc_auc\")\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944938e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dfe86d0",
   "metadata": {},
   "source": [
    "#### 2. Build Preprocessing + Baseline Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905bd48",
   "metadata": {},
   "source": [
    "`ColumnTransformer`\n",
    "- Numeric -> Impute + StandardScaler\n",
    "- Categorical -> Impute + OneHotEncoder\n",
    "\n",
    "`Model`\n",
    "- Logistic Regression (Classification)\n",
    "- Ridge (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### detect numeric vs. categorical columns from X.dtypes\n",
    "\n",
    "# remember to EXCLUDE ID column\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# categorical data: includes object/ string, bool, category, etc.\n",
    "cat_cols = X.select_dtypes(include=\"O\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf704b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")), # robust choice: impute missing values with median\n",
    "    (\"scalar\", StandardScalar()) # important for linear models: Logistic Regression, Ridge, etc.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e75766",
   "metadata": {},
   "source": [
    "`One Hot Encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"), # impute missing values with most frequently seen category\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))), # prevents errors when new categories appear\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00d8de",
   "metadata": {},
   "source": [
    "`Ordinal Encoder`\n",
    "\n",
    "- It maps each category to an integer (e.g. red->0, blue->1 )\n",
    "- works well -> when categories are truly ordered (e.g. low < medium < high)\n",
    "- risks -> when introduce fake \"order\" for unordered categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2908e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\", # drop any columns not specified above\n",
    "    verbose_feature_names_out=False # nicer feature names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"classification\":\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "if TASK == \"regression\":\n",
    "    model = Ridge(alpha=1.0, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de26d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686254a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Cross Validation\n",
    "- (stratified/ time/ group) CV with proper metrics (ROC-AUC, PR-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7da6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83694a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-series\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# group\n",
    "cv = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "# classification problem\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c78c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scoring(task: str, y_train: pd.Series) -> Dict[str, Any]:\n",
    "    if task == \"regression\":\n",
    "        return {\n",
    "            \"mae\": \"neg_mean_absolute_error\",\n",
    "            \"rmse\": \"neg_root_mean_squared_error\",\n",
    "            \"r2\": \"r2\",\n",
    "        }\n",
    "\n",
    "    # For binary: PR-AUC and ROC-AUC are common. For multiclass, fallback to logloss + f1_macro.\n",
    "    n_classes = y_train.nunique(dropna=True)\n",
    "    if n_classes == 2:\n",
    "        return {\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "            \"pr_auc\": \"average_precision\",\n",
    "            \"logloss\": \"neg_log_loss\",\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"f1_macro\": \"f1_macro\",\n",
    "            \"logloss\": \"neg_log_loss\",\n",
    "        }\n",
    "\n",
    "scoring = make_scoring(TASK, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c43b9c2",
   "metadata": {},
   "source": [
    "- True targets: $y_i$\n",
    "- Predictions: $\\hat{y}_i$\n",
    "- Number of samples: $n$\n",
    "- Mean of true targets: $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i$\n",
    "- Binary classification: $y_i\\in{0,1}$, predicted probability: $\\hat{p}_i=P(y_i=1\\mid x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ef106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathrm{MAE}=\\frac{1}{n}\\sum_{i=1}^{n}\\left|y_i-\\hat{y}_i\\right|$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathrm{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\hat{y}_i\\right)^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle R^2=1-\\frac{\\sum_{i=1}^{n}\\left(y_i-\\hat{y}_i\\right)^2}{\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathrm{Precision}=\\frac{TP}{TP+FP}\\quad,\\quad \\mathrm{Recall}=\\frac{TP}{TP+FN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathrm{F1}=\\frac{2\\cdot \\mathrm{Precision}\\cdot \\mathrm{Recall}}{\\mathrm{Precision}+\\mathrm{Recall}}=\\frac{2TP}{2TP+FP+FN}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathrm{LogLoss}=-\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_i\\log(\\hat{p}_i)+(1-y_i)\\log(1-\\hat{p}_i)\\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Markdown\n",
    "display(Math(r\"\\mathrm{MAE}=\\frac{1}{n}\\sum_{i=1}^{n}\\left|y_i-\\hat{y}_i\\right|\"))\n",
    "display(Math(r\"\\mathrm{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-\\hat{y}_i\\right)^2\"))\n",
    "display(Math(r\"R^2=1-\\frac{\\sum_{i=1}^{n}\\left(y_i-\\hat{y}_i\\right)^2}{\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2}\"))\n",
    "display(Math(r\"\\mathrm{Precision}=\\frac{TP}{TP+FP}\\quad,\\quad \\mathrm{Recall}=\\frac{TP}{TP+FN}\"))\n",
    "display(Math(r\"\\mathrm{F1}=\\frac{2\\cdot \\mathrm{Precision}\\cdot \\mathrm{Recall}}{\\mathrm{Precision}+\\mathrm{Recall}}=\\frac{2TP}{2TP+FP+FN}\"))\n",
    "display(Math(r\"\\mathrm{LogLoss}=-\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_i\\log(\\hat{p}_i)+(1-y_i)\\log(1-\\hat{p}_i)\\right]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_cv = groups_train if split_type == \"group\" else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    pipe,                    # CV will refit preprocessors + model inside each fold\n",
    "    X_train,                 # CV will further split train data into K folds internally\n",
    "    y_train,\n",
    "    cv=cv,                   # CV will the splitting strategy (e.g. KFold, StratifiedFold, TimeSeriesSplit, GroupKFold, etc.)\n",
    "    scoring=scoring,         # If a list or dict: multiple metrics (common)\n",
    "    n_jobs=-1,               # parallelize across CPU cores (use all cores available), speed up CV, especially with many folds/ models\n",
    "    groups=groups_cv,        # Only if CV splitter is group-aware (e.g. GroupKFold, StratifiedGroupKFold, GroupShuffleSplit)\n",
    "    return_train_score=False # If True, will also get train scores per fold -> useful to diagnose overfitting, but adds compute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23661083",
   "metadata": {},
   "source": [
    "for each fold split produced by cv\n",
    "- split X_train, y_train into (train_fold, valid_fold)\n",
    "- Fit the pipeline on train_fold\n",
    "- `pip` includes preprocessing (`ColumnTransformer`: imputing/ scaling/ encoding) + `model`\n",
    "- preprocessing is fit only on the fold's training data -> no leakage\n",
    "- score on valid_fold using requested `scoring` metrics\n",
    "- store each fold's scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba2390",
   "metadata": {},
   "source": [
    "### Fit Baseline on Train -> Evaluate on Holdout + Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393f6a3",
   "metadata": {},
   "source": [
    "for each fold split produced by cv\n",
    "- split X_train, y_train into (train_fold, valid_fold)\n",
    "- Fit the pipeline on train_fold\n",
    "- `pip` includes preprocessing (`ColumnTransformer`: imputing/ scaling/ encoding) + `model`\n",
    "- preprocessing is fit only on the fold's training data -> no leakage\n",
    "- score on valid_fold using requested `scoring` metrics\n",
    "- store each fold's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32579189",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"regression\":\n",
    "    pred = pipe.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    rmse = mean_squared_error(y_test, pred)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    \n",
    "    # Error analysis: worst absolute errors\n",
    "    error = np.abs(pred - y_test.values)\n",
    "    worst = np.argsort(-err)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28915db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"classification\":\n",
    "    proba = pipe.predict_proba(X_test)\n",
    "    \n",
    "    # Binary probability\n",
    "    if proba.shape[1] == 2:\n",
    "        p1 = proba[:, 1]\n",
    "        roc = roc_auc_score(y_test, p1)\n",
    "        pr = average_precision_score(y_test, p1)\n",
    "        ll = log_loss(y_test, p1)\n",
    "        \n",
    "        # Threshold setting\n",
    "        th = 0.5\n",
    "        preds = (p1 >= th).astype(int)\n",
    "        confusion_matrix(y_test, preds)\n",
    "        \n",
    "    # Multi-class probability\n",
    "    else:\n",
    "        preds = pipe.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "        ll = log_loss(y_test, proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
